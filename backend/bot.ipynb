{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResponseError",
     "evalue": "timed out waiting for llama runner to start: ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResponseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeepseek-r1:1.5b\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Change to the desired model (e.g., \"llama2\", \"gemma\")\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Query the model\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mollama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeaning of name shevarthana?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Print response\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Varsha K\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ollama\\_client.py:177\u001b[0m, in \u001b[0;36mClient.chat\u001b[1;34m(self, model, messages, stream, format, options, keep_alive)\u001b[0m\n\u001b[0;32m    174\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;241m:=\u001b[39m message\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    175\u001b[0m     message[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [_encode_image(image) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m--> 177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/api/chat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m  \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mformat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkeep_alive\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m  \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m  \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Varsha K\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ollama\\_client.py:97\u001b[0m, in \u001b[0;36mClient._request_stream\u001b[1;34m(self, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_request_stream\u001b[39m(\n\u001b[0;32m     92\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     93\u001b[0m   \u001b[38;5;241m*\u001b[39margs,\n\u001b[0;32m     94\u001b[0m   stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     95\u001b[0m   \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     96\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Mapping[\u001b[38;5;28mstr\u001b[39m, Any], Iterator[Mapping[\u001b[38;5;28mstr\u001b[39m, Any]]]:\n\u001b[1;32m---> 97\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[1;32mc:\\Users\\Varsha K\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ollama\\_client.py:73\u001b[0m, in \u001b[0;36mClient._request\u001b[1;34m(self, method, url, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m   response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 73\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[1;31mResponseError\u001b[0m: timed out waiting for llama runner to start: "
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "# Pull and load the model (e.g., Mistral or Llama2)\n",
    "model_name = \"deepseek-r1:1.5b\"  # Change to the desired model (e.g., \"llama2\", \"gemma\")\n",
    "\n",
    "# Query the model\n",
    "response = ollama.chat(model=model_name, messages=[{\"role\": \"user\", \"content\": \"meaning of name shevarthana?\"}])\n",
    "\n",
    "# Print response\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Preprocess ECG Data\n",
    "def preprocess_ecg_data(csv_file):\n",
    "    # Load the CSV file\n",
    "    data = pd.read_csv(csv_file, header=None)\n",
    "    time = data.iloc[:, 0].values\n",
    "    ecg_signal = data.iloc[:, 1].values\n",
    "    \n",
    "    # Simulate peak detection (replace with actual peak detection logic)\n",
    "    def detect_r_peaks(signal):\n",
    "        # Example: Simple threshold-based peak detection\n",
    "        threshold = np.mean(signal) + 0.5 * np.std(signal)\n",
    "        peaks = np.where(signal > threshold)[0]\n",
    "        return peaks\n",
    "    \n",
    "    r_peaks = detect_r_peaks(ecg_signal)\n",
    "    \n",
    "    # Extract features\n",
    "    features = {\n",
    "        'highest_point': np.max(ecg_signal),\n",
    "        'lowest_point': np.min(ecg_signal),\n",
    "        'avg_r_points': np.mean(ecg_signal[r_peaks]),\n",
    "        'rr_interval': np.mean(np.diff(time[r_peaks])) if len(r_peaks) > 1 else 0,\n",
    "        'qrs_duration': 0.08,  # Example value (replace with actual calculation)\n",
    "        'qt_duration': 0.36,  # Example value (replace with actual calculation)\n",
    "    }\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Step 2: Generate Summary Using Ollama\n",
    "def generate_summary(features):\n",
    "    # Prepare the prompt for the model\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following ECG data:\n",
    "    - Highest Point: {features['highest_point']}\n",
    "    - Lowest Point: {features['lowest_point']}\n",
    "    - Average R-Points: {features['avg_r_points']}\n",
    "    - RR Interval: {features['rr_interval']}\n",
    "    - QRS Duration: {features['qrs_duration']}\n",
    "    - QT Duration: {features['qt_duration']}\n",
    "\n",
    "    Summarize the ECG data and provide insights into possible cardiac conditions.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Query the Ollama model\n",
    "    response = ollama.chat(\n",
    "        model=\"medllama2\",  # Use the medical-focused model\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    return response[\"message\"][\"content\"]\n",
    "\n",
    "# Step 3: Handle User Queries\n",
    "def handle_user_query(query, features):\n",
    "    # Prepare the prompt for the model\n",
    "    prompt = f\"\"\"\n",
    "    ECG Data Features:\n",
    "    - Highest Point: {features['highest_point']}\n",
    "    - Lowest Point: {features['lowest_point']}\n",
    "    - Average R-Points: {features['avg_r_points']}\n",
    "    - RR Interval: {features['rr_interval']}\n",
    "    - QRS Duration: {features['qrs_duration']}\n",
    "    - QT Duration: {features['qt_duration']}\n",
    "\n",
    "    User Query: {query}\n",
    "\n",
    "    Provide a detailed response to the user's query based on the ECG data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Query the Ollama model\n",
    "    response = ollama.chat(\n",
    "        model=\"medllama2\",  # Use the medical-focused model\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    return response[\"message\"][\"content\"]\n",
    "\n",
    "# Step 4: Main Workflow\n",
    "def main(csv_file, user_query=None):\n",
    "    # Preprocess ECG data\n",
    "    features = preprocess_ecg_data(csv_file)\n",
    "    \n",
    "    # Generate summary\n",
    "    summary = generate_summary(features)\n",
    "    print(\"ECG Summary:\\n\", summary)\n",
    "    \n",
    "    # Handle user query if provided\n",
    "    if user_query:\n",
    "        response = handle_user_query(user_query, features)\n",
    "        print(\"\\nUser Query Response:\\n\", response)\n",
    "\n",
    "# Example Usage\n",
    "csv_file = \"ecg_data.csv\"  # Replace with your CSV file path\n",
    "user_query = \"Are there any signs of blockage in the ECG data?\"\n",
    "main(csv_file, user_query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
